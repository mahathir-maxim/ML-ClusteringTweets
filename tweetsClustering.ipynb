{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweetsClustering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvyn1-kO4j1H",
        "outputId": "72fcb71b-f684-4d89-b1ec-68797d30e5f7"
      },
      "source": [
        "import sys\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import copy\n",
        "import math\n",
        "import random as rd\n",
        "import urllib.request\n",
        "\n",
        "def pre_process_tweets(url):\n",
        "\n",
        "    tweets = []\n",
        "    \n",
        "    rawData = urllib.request.urlopen(\"https://github.com/mahathir-maxim/ML-ClusteringTweets/blob/main/bbchealth.txt?raw=true\")\n",
        "    dataLines = rawData.readlines()\n",
        "\n",
        "    for line in dataLines:\n",
        "        decodedLine = line.decode(\"utf-8\")\n",
        "        tweets.append(decodedLine)\n",
        "\n",
        "    list_of_tweets = []\n",
        "\n",
        "    for i in range(len(tweets)):\n",
        "\n",
        "        # remove \\n from the end after every sentence\n",
        "        tweets[i] = tweets[i].strip('\\n')\n",
        "\n",
        "        # Remove the tweet id and timestamp\n",
        "        tweets[i] = tweets[i][50:]\n",
        "\n",
        "        # Remove any word that starts with the symbol @\n",
        "        tweets[i] = \" \".join(filter(lambda x: x[0] != '@', tweets[i].split()))\n",
        "\n",
        "        # Remove any URL\n",
        "        tweets[i] = re.sub(r\"http\\S+\", \"\", tweets[i])\n",
        "        tweets[i] = re.sub(r\"www\\S+\", \"\", tweets[i])\n",
        "\n",
        "        # remove colons from the end of the sentences (if any) after removing url\n",
        "        tweets[i] = tweets[i].strip()\n",
        "        tweet_len = len(tweets[i])\n",
        "        if tweet_len > 0:\n",
        "            if tweets[i][len(tweets[i]) - 1] == ':':\n",
        "                tweets[i] = tweets[i][:len(tweets[i]) - 1]\n",
        "\n",
        "        # Remove any hash-tags symbols\n",
        "        tweets[i] = tweets[i].replace('#', '')\n",
        "\n",
        "        # Convert every word to lowercase\n",
        "        tweets[i] = tweets[i].lower()\n",
        "\n",
        "        # remove punctuations\n",
        "        tweets[i] = tweets[i].translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # trim extra spaces\n",
        "        tweets[i] = \" \".join(tweets[i].split())\n",
        "\n",
        "        # convert each tweet from string type to as list<string> using \" \" as a delimiter\n",
        "        list_of_tweets.append(tweets[i].split(' '))\n",
        "\n",
        "\n",
        "    return list_of_tweets\n",
        "\n",
        "\n",
        "def k_means(tweets, k=4, max_iterations=50):\n",
        "\n",
        "    centroids = []\n",
        "\n",
        "    # initialization, assign random tweets as centroids\n",
        "    count = 0\n",
        "    hash_map = dict()\n",
        "    while count < k:\n",
        "        random_tweet_idx = rd.randint(0, len(tweets) - 1)\n",
        "        if random_tweet_idx not in hash_map:\n",
        "            count += 1\n",
        "            hash_map[random_tweet_idx] = True\n",
        "            centroids.append(tweets[random_tweet_idx])\n",
        "\n",
        "    iter_count = 0\n",
        "    prev_centroids = []\n",
        "\n",
        "    # run the iterations until not converged or until the max iteration in not reached\n",
        "    while (is_converged(prev_centroids, centroids)) == False and (iter_count < max_iterations):\n",
        "\n",
        "        print(\"running iteration \" + str(iter_count))\n",
        "\n",
        "        # assignment, assign tweets to the closest centroids\n",
        "        clusters = assign_cluster(tweets, centroids)\n",
        "\n",
        "        # to check if k-means converges, keep track of prev_centroids\n",
        "        prev_centroids = centroids\n",
        "\n",
        "        # update, update centroid based on clusters formed\n",
        "        centroids = update_centroids(clusters)\n",
        "        iter_count = iter_count + 1\n",
        "\n",
        "    if (iter_count == max_iterations):\n",
        "        print(\"max iterations reached, K means not converged\")\n",
        "    else:\n",
        "        print(\"converged\")\n",
        "\n",
        "    sse = compute_SSE(clusters)\n",
        "\n",
        "    return clusters, sse\n",
        "\n",
        "\n",
        "def is_converged(prev_centroid, new_centroids):\n",
        "\n",
        "    # false if lengths are not equal\n",
        "    if len(prev_centroid) != len(new_centroids):\n",
        "        return False\n",
        "\n",
        "    # iterate over each entry of clusters and check if they are same\n",
        "    for c in range(len(new_centroids)):\n",
        "        if \" \".join(new_centroids[c]) != \" \".join(prev_centroid[c]):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def assign_cluster(tweets, centroids):\n",
        "\n",
        "    clusters = dict()\n",
        "\n",
        "    # for every tweet iterate each centroid and assign closest centroid to a it\n",
        "    for t in range(len(tweets)):\n",
        "        min_dis = math.inf\n",
        "        cluster_idx = -1;\n",
        "        for c in range(len(centroids)):\n",
        "            dis = getDistance(centroids[c], tweets[t])\n",
        "            # look for a closest centroid for a tweet\n",
        "\n",
        "            if centroids[c] == tweets[t]:\n",
        "                # print(\"tweet and centroid are equal with c: \" + str(c) + \", t\" + str(t))\n",
        "                cluster_idx = c\n",
        "                min_dis = 0\n",
        "                break\n",
        "\n",
        "            if dis < min_dis:\n",
        "                cluster_idx = c\n",
        "                min_dis = dis\n",
        "\n",
        "        # randomise the centroid assignment to a tweet if nothing is common\n",
        "        if min_dis == 1:\n",
        "            cluster_idx = rd.randint(0, len(centroids) - 1)\n",
        "\n",
        "        # assign the closest centroid to a tweet\n",
        "        clusters.setdefault(cluster_idx, []).append([tweets[t]])\n",
        "        # print(\"tweet t: \" + str(t) + \" is assigned to cluster c: \" + str(cluster_idx))\n",
        "        # add the tweet distance from its closest centroid to compute sse in the end\n",
        "        last_tweet_idx = len(clusters.setdefault(cluster_idx, [])) - 1\n",
        "        clusters.setdefault(cluster_idx, [])[last_tweet_idx].append(min_dis)\n",
        "\n",
        "    return clusters\n",
        "\n",
        "\n",
        "def update_centroids(clusters):\n",
        "\n",
        "    centroids = []\n",
        "\n",
        "    # iterate each cluster and check for a tweet with closest distance sum with all other tweets in the same cluster\n",
        "    # select that tweet as the centroid for the cluster\n",
        "    for c in range(len(clusters)):\n",
        "        min_dis_sum = math.inf\n",
        "        centroid_idx = -1\n",
        "\n",
        "        # to avoid redundant calculations\n",
        "        min_dis_dp = []\n",
        "\n",
        "        for t1 in range(len(clusters[c])):\n",
        "            min_dis_dp.append([])\n",
        "            dis_sum = 0\n",
        "            # get distances sum for every of tweet t1 with every tweet t2 in a same cluster\n",
        "            for t2 in range(len(clusters[c])):\n",
        "                if t1 != t2:\n",
        "                    if t2 < t1:\n",
        "                        dis = min_dis_dp[t2][t1]\n",
        "                    else:\n",
        "                        dis = getDistance(clusters[c][t1][0], clusters[c][t2][0])\n",
        "\n",
        "                    min_dis_dp[t1].append(dis)\n",
        "                    dis_sum += dis\n",
        "                else:\n",
        "                    min_dis_dp[t1].append(0)\n",
        "\n",
        "            # select the tweet with the minimum distances sum as the centroid for the cluster\n",
        "            if dis_sum < min_dis_sum:\n",
        "                min_dis_sum = dis_sum\n",
        "                centroid_idx = t1\n",
        "\n",
        "        # append the selected tweet to the centroid list\n",
        "        centroids.append(clusters[c][centroid_idx][0])\n",
        "\n",
        "    return centroids\n",
        "\n",
        "\n",
        "def getDistance(tweet1, tweet2):\n",
        "\n",
        "    # get the intersection\n",
        "    intersection = set(tweet1).intersection(tweet2)\n",
        "\n",
        "    # get the union\n",
        "    union = set().union(tweet1, tweet2)\n",
        "\n",
        "    # return the jaccard distance\n",
        "    return 1 - (len(intersection) / len(union))\n",
        "\n",
        "\n",
        "def compute_SSE(clusters):\n",
        "\n",
        "    sse = 0\n",
        "    # iterate every cluster 'c', compute SSE as the sum of square of distances of the tweet from it's centroid\n",
        "    for c in range(len(clusters)):\n",
        "        for t in range(len(clusters[c])):\n",
        "            sse = sse + (clusters[c][t][1] * clusters[c][t][1])\n",
        "\n",
        "    return sse\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    data_url = 'Health_Tweets/bbchealth.txt'\n",
        "\n",
        "    tweets = pre_process_tweets(data_url)\n",
        "\n",
        "    # default number of experiments to be performed\n",
        "    experiments = 5\n",
        "\n",
        "    # default value of K for K-means\n",
        "    k = 3\n",
        "\n",
        "    # for every experiment 'e', run K-means\n",
        "    for e in range(experiments):\n",
        "\n",
        "        print(\"------ Running K means for experiment no. \" + str((e + 1)) + \" for k = \" + str(k))\n",
        "\n",
        "        clusters, sse = k_means(tweets, k)\n",
        "\n",
        "        # for every cluster 'c', print size of each cluster\n",
        "        for c in range(len(clusters)):\n",
        "            print(str(c+1) + \": \", str(len(clusters[c])) + \" tweets\")\n",
        "            # # to print tweets in a cluster\n",
        "            # for t in range(len(clusters[c])):\n",
        "            #     print(\"t\" + str(t) + \", \" + (\" \".join(clusters[c][t][0])))\n",
        "\n",
        "        print(\"--> SSE : \" + str(sse))\n",
        "        print('\\n')\n",
        "\n",
        "        # increment k after every experiment\n",
        "        k = k + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Running K means for experiment no. 1 for k = 3\n",
            "running iteration 0\n",
            "running iteration 1\n",
            "converged\n",
            "1:  1302 tweets\n",
            "2:  1774 tweets\n",
            "3:  853 tweets\n",
            "--> SSE : 3395.105988508928\n",
            "\n",
            "\n",
            "------ Running K means for experiment no. 2 for k = 4\n",
            "running iteration 0\n",
            "running iteration 1\n",
            "converged\n",
            "1:  949 tweets\n",
            "2:  1012 tweets\n",
            "3:  1034 tweets\n",
            "4:  934 tweets\n",
            "--> SSE : 3342.7379220697976\n",
            "\n",
            "\n",
            "------ Running K means for experiment no. 3 for k = 5\n",
            "running iteration 0\n",
            "running iteration 1\n",
            "converged\n",
            "1:  1249 tweets\n",
            "2:  887 tweets\n",
            "3:  611 tweets\n",
            "4:  491 tweets\n",
            "5:  691 tweets\n",
            "--> SSE : 3346.9200584762943\n",
            "\n",
            "\n",
            "------ Running K means for experiment no. 4 for k = 6\n",
            "running iteration 0\n",
            "running iteration 1\n",
            "running iteration 2\n",
            "running iteration 3\n",
            "converged\n",
            "1:  773 tweets\n",
            "2:  914 tweets\n",
            "3:  626 tweets\n",
            "4:  312 tweets\n",
            "5:  937 tweets\n",
            "6:  367 tweets\n",
            "--> SSE : 3356.3332569811\n",
            "\n",
            "\n",
            "------ Running K means for experiment no. 5 for k = 7\n",
            "running iteration 0\n",
            "running iteration 1\n",
            "converged\n",
            "1:  670 tweets\n",
            "2:  375 tweets\n",
            "3:  707 tweets\n",
            "4:  434 tweets\n",
            "5:  411 tweets\n",
            "6:  817 tweets\n",
            "7:  515 tweets\n",
            "--> SSE : 3296.6344361826295\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}